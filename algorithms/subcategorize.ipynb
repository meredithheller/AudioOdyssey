{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_description(description):    \n",
    "    description = re.sub(r'http\\S+', '', description)\n",
    "    description = re.sub(r'\\S*@\\S*\\s?', '', description)\n",
    "    description = re.sub(r'#', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])podcast(?![^ .:,?!;\\r\\n])', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])podcasts(?![^ .,:?!;\\r\\n])', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])host(?![^ .,?!:;\\r\\n])', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])hosts(?![^ .,?!;\\r\\n])', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])episode(?![^ .,:?!;\\r\\n])', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])episodes(?![^ .:,?!;\\r\\n])', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])sponsor(?![^ .,:?!;\\r\\n])', '', description)\n",
    "    description = re.sub(r'(?<![^ .,?!;])sponsored(?![^ .:,?!;\\r\\n])', '', description)\n",
    "    return description.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# get only the necessary columns from the data and convert to a dataframe\n",
    "podcasts = pandas.read_csv('metadata.tsv', sep='\\t', usecols = ['show_uri', 'show_name', 'language', 'show_description', 'rss_link', 'episode_uri', 'episode_name',\t'episode_description',\t'duration'], nrows=12000)\n",
    "\n",
    "# only include podcasts in english language\n",
    "eng_abr = set((\"['en']\", \"['en-au']\", \"['en-bz'}\", \"['en-ca']\", \"['en-ie']\", \"['en-jm']\", \"['en-nz']\", \"['en-za']\", \"['en-tt']\", \"['en-gb']\", \"['en-us']\"))\n",
    "\n",
    "# ensure all values in language column are lowercase\n",
    "podcasts['language'] = podcasts['language'].str.lower()\n",
    "\n",
    "# create dataframe of english podcasts\n",
    "eng_podcasts = podcasts[podcasts['language'].isin(eng_abr)]\n",
    "eng_podcasts = eng_podcasts.drop(['language'], axis=1)\n",
    "eng_podcasts = eng_podcasts[eng_podcasts['episode_description'].apply(lambda x: isinstance(x, str))]\n",
    "eng_podcasts = eng_podcasts[eng_podcasts['show_description'].apply(lambda x: isinstance(x, str))]\n",
    "eng_podcasts['image_url'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the podcasts and create category dataframe\n",
    "# open all xml files in show-rss\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "def parseRSS(rss_link):\n",
    "   success = False\n",
    "   ns = {'itunes': 'http://www.itunes.com/dtds/podcast-1.0.dtd'}\n",
    "   try:\n",
    "      resp = requests.get(rss_link)\n",
    "   except:\n",
    "      resp = None\n",
    "   if resp != None:\n",
    "      with open('podcast.xml', 'wb') as f:\n",
    "         f.write(resp.content)\n",
    "      categories = set()\n",
    "      image_url = None\n",
    "      try:\n",
    "         # create element tree object\n",
    "         tree = ET.parse('podcast.xml')\n",
    "         # get root element\n",
    "         root = tree.getroot()\n",
    "         for child in root.iter():\n",
    "            for cat in child.findall('itunes:category', ns):\n",
    "               category = cat.get('text')\n",
    "               categories.add(category)\n",
    "            image_tag = child.findall('itunes:image', ns)\n",
    "            if image_tag:\n",
    "               image_url = image_tag[0].get('href')\n",
    "         if len(categories) > 0 and image_url and ('http' in image_url) and ('.jpg' in image_url or '.png' in image_url):\n",
    "            success = True\n",
    "      except:\n",
    "         pass\n",
    "      finally:\n",
    "         if os.path.exists(\"podcast.xml\"):\n",
    "            os.remove(\"podcast.xml\")\n",
    "         if success:\n",
    "            return (categories, image_url)\n",
    "   return (None, None)\n",
    "\n",
    "\n",
    "# create a list of categories and the associated podcast\n",
    "cat_list = list()\n",
    "count = 0\n",
    "all_categories = set()\n",
    "indices_to_remove = []\n",
    "# iterate through all columns in dataframe\n",
    "for ind in eng_podcasts.index:\n",
    "   if count < 10000:\n",
    "      uri = eng_podcasts['episode_uri'][ind]\n",
    "      (categories, image_url) = parseRSS(eng_podcasts['rss_link'][ind])\n",
    "      if categories != None and image_url != None:\n",
    "         count += 1\n",
    "         for cat in categories:\n",
    "            all_categories.add(cat)\n",
    "            cat_list.append({'episode_uri': uri, 'category': cat})\n",
    "         eng_podcasts.loc[ind, 'image_url'] = image_url\n",
    "      else:\n",
    "         indices_to_remove.append(ind)\n",
    "   else:\n",
    "      indices_to_remove.append(ind)\n",
    "\n",
    "# remove the rows in dataframe that were not succesful\n",
    "# eng_podcasts = eng_podcasts.drop(eng_podcasts.index[indices_to_remove])\n",
    "            \n",
    "# convert list of categories and podcast to dataframe\n",
    "cat_df = pandas.DataFrame.from_records(cat_list)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subcategories dataframe\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "\n",
    "subcat_list = list()\n",
    "for index in eng_podcasts.index:\n",
    "    episode_uri = eng_podcasts['episode_uri'][index]\n",
    "    episode_desc = clean_description(eng_podcasts['episode_description'][index])\n",
    "    show_desc = clean_description(eng_podcasts['show_description'][index])\n",
    "    episode_words = set(episode_desc.split(' '))\n",
    "    show_words = set(show_desc.split(' '))\n",
    "    descriptions = episode_desc + episode_desc + episode_desc + show_desc\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(descriptions)\n",
    "    key_phrases = r.get_ranked_phrases_with_scores()\n",
    "    count_subcategories = 0\n",
    "    cur_score = 0\n",
    "    cur_subcategories = set()\n",
    "    while key_phrases:\n",
    "        cur_phrase = key_phrases.pop(0)\n",
    "        if count_subcategories >= 10 or (count_subcategories >= 5 and cur_score != cur_phrase[0]):\n",
    "            break\n",
    "        cur_score = cur_phrase[0]\n",
    "        if cur_phrase[1] in cur_subcategories:\n",
    "            continue\n",
    "        cur_subcategories.add(cur_phrase[1].rstrip())\n",
    "        is_power_subcategory = False\n",
    "        if cur_phrase[1].lower() in show_desc and cur_phrase[1] in episode_desc:\n",
    "            is_power_subcategory = True\n",
    "        new_item = {'episode_uri': episode_uri, 'subcategory': cur_phrase[1].rstrip(), 'is_power': is_power_subcategory}\n",
    "        subcat_list.append(new_item)\n",
    "        count_subcategories += 1\n",
    "\n",
    "subcat_df = pandas.DataFrame.from_records(subcat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9.0, 'play center midfield'), (8.5, 'soccer ahs long'), (4.5, 'soccer team'), (4.0, 'favorite sport'), (1.0, 'work'), (1.0, 'stuff'), (1.0, 'name'), (1.0, 'meredith'), (1.0, 'ideas'), (1.0, 'hello'), (1.0, 'curious')]\n"
     ]
    }
   ],
   "source": [
    "descriptions = 'Hello my name is Meredith. I am very curious why my stuff will not work. Do you have any ideas? Soccer ahs long been my favorite sport. I play center midfield on my soccer team.'\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(descriptions)\n",
    "key_phrases = r.get_ranked_phrases_with_scores()\n",
    "print(key_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16376"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "hostname=\"localhost\"\n",
    "dbname=\"mheller5\"\n",
    "uname=\"mheller5\"\n",
    "pwd=\"audioodyssey\"\n",
    "\n",
    "podcast_df = eng_podcasts.drop(['rss_link', 'episode_description', 'show_description', 'image_url'], axis=1)\n",
    "# make episode description table\n",
    "desc_df = eng_podcasts[['episode_uri', 'episode_description']]\n",
    "image_df = eng_podcasts[['episode_uri', 'image_url']]\n",
    "\n",
    "\n",
    "# Create SQLAlchemy engine to connect to MySQL Database\n",
    "engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "\t\t\t\t.format(host=hostname, db=dbname, user=uname, pw=pwd))\n",
    "\n",
    "# Convert dataframe to sql table                                   \n",
    "podcast_df.to_sql('podcasts', engine, index=False, dtype={\n",
    "\t# 'episode_uri': sqlalchemy.types.NVARCHAR(length=40),\n",
    "\t# 'show_uri': sqlalchemy.types.NVARCHAR(length=40),\n",
    "\t# 'show_name': sqlalchemy.types.NVARCHAR(length=255),\n",
    "\t# 'episode_name': sqlalchemy.types.NVARCHAR(length=255),\n",
    "\t# 'duration': sqlalchemy.types.FLOAT, # TODO: need to figure out this type\n",
    "\t# 'rss_link': sqlalchemy.types.NVARCHAR(length=70)\n",
    "})\n",
    "desc_df.to_sql('descriptions', engine, dtype={'episode_uri': sqlalchemy.types.NVARCHAR(length=40)}, index=False)\n",
    "image_df.to_sql('image_urls', engine, dtype={'episode_uri': sqlalchemy.types.NVARCHAR(length=40)}, index=False)\n",
    "cat_df.to_sql('categories', engine, index=False, dtype={\n",
    "\t'episode_uri': sqlalchemy.types.NVARCHAR(length=40),\n",
    "\t'category': sqlalchemy.types.NVARCHAR(length=40)\n",
    "})\n",
    "# subcat_df.to_sql('subcategories', engine, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63830"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "hostname=\"localhost\"\n",
    "dbname=\"mheller5\"\n",
    "uname=\"mheller5\"\n",
    "pwd=\"audioodyssey\"\n",
    "\n",
    "# Create SQLAlchemy engine to connect to MySQL Database\n",
    "engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "\t\t\t\t.format(host=hostname, db=dbname, user=uname, pw=pwd))\n",
    "\n",
    "# make sure all subcats are strings\n",
    "subcat_df = subcat_df[subcat_df['subcategory'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "subcat_df.to_sql('subcategories', engine, index=False, dtype={\n",
    "\t'episode_uri': sqlalchemy.types.NVARCHAR(length=40),\n",
    "\t'is_power': sqlalchemy.types.BOOLEAN\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffc8adf714ed33ec6159892d519aa5984636c14dac7516ab103ed7138df97175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
